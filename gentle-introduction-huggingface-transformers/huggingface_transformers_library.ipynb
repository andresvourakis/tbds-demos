{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Gentle Introduction to Hugging Face Transformers Library\n",
        "===\n",
        "\n",
        "This notebook demonstrates the code examples from this [article](https://tobeadatascientist.substack.com/p/gentle-introduction-to-hugging-face-transformers), showcasing the before and after of each technique.\n",
        "\n",
        "For more resources like this, visit [tobeadatascientist.com](https://tobeadatascientist.com)"
      ],
      "metadata": {
        "id": "72vg5HkvuNEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 1: Using the pipeline API"
      ],
      "metadata": {
        "id": "-PI4QSsUubcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the text-generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# Generate text\n",
        "prompt = \"Hugging Face makes NLP\"\n",
        "output = generator(prompt, max_length=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJBtcvnBtf_N",
        "outputId": "4eb090c0-71df-4bc1-a520-1d5ba848c10e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the generated text\n",
        "print(f\"Input: {prompt}\")\n",
        "print(f\"Generated Text: {output[0]['generated_text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr9DMBfpuYqa",
        "outputId": "5598508c-1ce9-4d0b-9c8c-448bba2eee0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Hugging Face makes NLP\n",
            "Generated Text: Hugging Face makes NLP better than I expected, it's my fault she didn't take a more important part of the game and pushed us out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 2: Using AutoTokenizer and AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "zQD7ZfVEukgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Prepare the input text\n",
        "prompt = \"Hugging Face makes NLP\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text\n",
        "outputs = model.generate(**inputs, max_length=30)\n",
        "\n",
        "# Decode the output\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KLjYRd7tiAp",
        "outputId": "b683a632-b97e-4377-a8df-486d5d9357a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the generated text\n",
        "print(f\"Input: {prompt}\")\n",
        "print(f\"Generated Text: {generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhLgY1wSuV-u",
        "outputId": "e1f4031e-5a49-4738-ae7b-017fce7c3984"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Hugging Face makes NLP\n",
            "Generated Text: Hugging Face makes NLP a great way to get your face into the game.\n",
            "\n",
            "The NLP is a great way to get your face\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Step 1: Simulate a user input\n",
        "user_input = \"Hello, I am looking for a bot to help me. Can you do that??\"\n",
        "\n",
        "# Step 2: Tokenize with truncation and padding\n",
        "inputs = tokenizer(\n",
        "    user_input,\n",
        "    max_length=30,  # Allow up to 30 tokens\n",
        "    padding=\"max_length\",  # Add padding if input is short\n",
        "    truncation=True,  # Truncate input if it's too long\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Step 3: Generate a response with controlled length\n",
        "output_ids = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    max_length=20,  # Limit response to 20 tokens\n",
        "    num_return_sequences=1,  # Generate one response\n",
        "    temperature=0.7,  # Add randomness for creative responses\n",
        ")\n",
        "\n",
        "# Step 4: Decode the generated tokens back to text\n",
        "response = tokenizer.decode(output_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "x7IOepz0wYcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the input and chatbot's response\n",
        "print(\"User Input:\", user_input)\n",
        "print(\"\\nChatbot Response:\", response)"
      ],
      "metadata": {
        "id": "6Nb4VeXXwh6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Find more information in the official [documentation](https://huggingface.co/docs/transformers/en/index)*"
      ],
      "metadata": {
        "id": "cn_tARE4vGCy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGock_0ivLVG"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}