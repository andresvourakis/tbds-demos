{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Gentle Introduction to Hugging Face Transformers Library\n",
        "===\n",
        "\n",
        "This notebook demonstrates the code examples from this [article](https://tobeadatascientist.substack.com/p/gentle-introduction-to-hugging-face-transformers), showcasing the before and after of each technique.\n",
        "\n",
        "For more resources like this, visit [tobeadatascientist.com](https://tobeadatascientist.com)"
      ],
      "metadata": {
        "id": "72vg5HkvuNEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 1: Using the pipeline API"
      ],
      "metadata": {
        "id": "-PI4QSsUubcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the text-generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# Generate text\n",
        "prompt = \"In Python, a list comprehension is a more concise way to\"\n",
        "output = generator(prompt, max_length=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJBtcvnBtf_N",
        "outputId": "e611122c-0d2d-4255-a06e-7e9b32d3f8be"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the generated text\n",
        "print(f\"Input: {prompt}\")\n",
        "print(f\"Generated text: {output[0]['generated_text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr9DMBfpuYqa",
        "outputId": "12d11681-03db-446b-c4cf-1207043ba90a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: In Python, a list comprehension is a more concise way to\n",
            "Generated text: In Python, a list comprehension is a more concise way to represent a single input item than an array of strings, as each and every element in the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 2: Using AutoTokenizer and AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "zQD7ZfVEukgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load the GPT-2 tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Set a padding token to avoid warnings (GPT-2 doesn't have one by default)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define the starting text\n",
        "input_text = \"In Python, a list comprehension is a more concise way to\"\n",
        "\n",
        "# Convert text into tokens (numbers the model can understand)\n",
        "tokens = tokenizer(\n",
        "    input_text,\n",
        "    return_tensors=\"pt\",  # Convert text into a PyTorch tensor (needed for the model)\n",
        "    truncation=True,  # If the input is too long, cut it down to max_length\n",
        "    max_length=50  # Limit input to 50 tokens\n",
        ")\n",
        "\n",
        "# Generate new text based on the input\n",
        "output_ids = model.generate(\n",
        "    tokens[\"input_ids\"],  # The model takes tokenized input\n",
        "    max_new_tokens=50,  # Generate up to 50 new tokens\n",
        "    do_sample=True,  # Enables randomness for more varied responses\n",
        "    temperature=0.8,  # Controls randomness (higher = more creative output)\n",
        "    top_k=50,  # Only consider the 50 most likely words at each step\n",
        "    top_p=0.9,  # Use nucleus sampling (pick words with a total probability of 90%)\n",
        "    repetition_penalty=1.2,  # Reduce repetitive phrases in the output\n",
        "    pad_token_id=tokenizer.eos_token_id  # Avoids errors related to missing padding\n",
        ")\n",
        "\n",
        "# Convert the generated tokens back into readable text\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "x7IOepz0wYcF"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the input and chatbot's response\n",
        "print(\"Input:\", input_text)\n",
        "print(\"\\nGenerated text:\", generated_text)"
      ],
      "metadata": {
        "id": "6Nb4VeXXwh6x",
        "outputId": "a35f4d1e-dde8-482c-a469-580f4e576aa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: In Python, a list comprehension is a more concise way to\n",
            "\n",
            "Generated text: In Python, a list comprehension is a more concise way to construct lists and the fact that it's written in C makes things even easier.\n",
            "It doesn't take much work (at least not for me), but you can make an iterator over every element of your program:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Find more information in the official [documentation](https://huggingface.co/docs/transformers/en/index)*"
      ],
      "metadata": {
        "id": "cn_tARE4vGCy"
      }
    }
  ]
}